{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.5 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")\n",
    "\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc_units=200):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc_units (int): Number of nodes in the hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc_units)\n",
    "        self.fc2 = nn.Linear(fc_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        return F.tanh(self.fc2(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=80, fc2_units=80, fc3_units=40):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "            fc3_units (int): Number of nodes in the third hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, fc3_units)\n",
    "        self.fc4 = nn.Linear(fc3_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.fc4.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        xs = F.leaky_relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(3e4)      # replay buffer size\n",
    "BATCH_SIZE = 512            # minibatch size\n",
    "GAMMA = 0.99                # discount factor\n",
    "TAU = 1e-2                  # for soft update of target parameters\n",
    "LR_ACTOR = 1e-3             # learning rate of the actor \n",
    "LR_CRITIC = 3e-3            # learning rate of the critic\n",
    "UPDATE_EVERY = 4            # Update weights after this amount of steps\n",
    "SUCCESS_REPLAY_FACTOR = 4   # Add successful actions to buffer multiple times\n",
    "MAIN_GAME_REPLAY_FACTOR = 2 # Add experiences after 1st hit multiple times to buffer\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.step_counter = 0\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done, main_game):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        self.step_counter += 1\n",
    "        \n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # If this is a successful move, add to replay memory multiple times\n",
    "        if reward > 0.0:\n",
    "            for i in range(SUCCESS_REPLAY_FACTOR - 1):\n",
    "                self.memory.add(state, action, reward, next_state, done)\n",
    "        # If we are in the main game (after 1st hit), add to replay memory multiple times\n",
    "        elif main_game:\n",
    "            for k in range(MAIN_GAME_REPLAY_FACTOR - 1):\n",
    "                self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if self.step_counter == UPDATE_EVERY:\n",
    "            self.step_counter = 0\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, states):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_local(states).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        return np.clip(actions, -1, 1)\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Updates policy and value parameters using given batch of experience tuples.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # update critic \n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # update actor \n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # update target networks\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        b_size = min(len(self.memory), self.batch_size)\n",
    "        experiences = random.sample(self.memory, k=b_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=48, action_size=4, random_seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500\tAverage Score: 0.01\tEpisode Score: 0.09\n",
      "Episode 1000\tAverage Score: 0.01\tEpisode Score: 0.00\n",
      "Episode 1500\tAverage Score: 0.04\tEpisode Score: 0.00\n",
      "Episode 2000\tAverage Score: 0.04\tEpisode Score: 0.10\n",
      "Episode 2500\tAverage Score: 0.15\tEpisode Score: 0.10\n",
      "Environment solved in 2605 episodes\tAverage Score: 0.51\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5a6d1d56a0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def ddpg(n_episodes=10000, max_t=2000):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations.flatten()\n",
    "        episode_scores = np.zeros(num_agents)\n",
    "        main_game = False\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state)\n",
    "            action_per_agent = action.reshape((2,2))\n",
    "            env_info = env.step(action_per_agent)[brain_name]\n",
    "            next_state = env_info.vector_observations.flatten()\n",
    "            rewards = np.array(env_info.rewards)    \n",
    "            done = np.any(env_info.local_done)\n",
    "            agent.step(state, action, rewards.sum(), next_state, done, main_game)\n",
    "            # The first time we hit the ball, we move into the main game.\n",
    "            if rewards.sum() > 0.0:\n",
    "                main_game = True\n",
    "            state = next_state\n",
    "            episode_scores += rewards\n",
    "            if done:\n",
    "                break\n",
    "        episode_score = np.max(episode_scores)\n",
    "        scores_deque.append(episode_score)\n",
    "        scores.append(episode_score)\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEpisode Score: {:.2f}'\\\n",
    "              .format(i_episode, np.mean(scores_deque), episode_score), end=\"\")\n",
    "        \n",
    "        if i_episode % 500 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))   \n",
    "            \n",
    "        if np.mean(scores_deque) >= 0.5:\n",
    "            torch.save(agent.actor_local.state_dict(), 'solution_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'solution_critic.pth')\n",
    "            print('\\rEnvironment solved in {} episodes\\tAverage Score: {:.2f}'\\\n",
    "                  .format(i_episode, np.mean(scores_deque)))\n",
    "            break\n",
    "    \n",
    "    return scores\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
